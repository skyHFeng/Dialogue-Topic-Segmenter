{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ec422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from segeval.window.pk import pk\n",
    "from segeval.window.windowdiff import window_diff as wd\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from transformers import BertForNextSentencePrediction\n",
    "import statistics\n",
    "from sklearn.metrics import mean_absolute_error, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e94bc8d",
   "metadata": {},
   "source": [
    "深度计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393ed69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_score_cal(scores):\n",
    "    output_scores = []\n",
    "\n",
    "    # 按一致性分数，遍历话语对（k-1个）\n",
    "    for i in range(len(scores)):\n",
    "        # 记录话语对i左侧的最大一致性分数\n",
    "        lflag = scores[i]\n",
    "        # 记录话语对i右侧的最大一致性分数\n",
    "        rflag = scores[i]\n",
    "\n",
    "        # 第一个话语对\n",
    "        if i == 0:\n",
    "            for r in range(i+1, len(scores)):\n",
    "                if rflag <= scores[r]:\n",
    "                    rflag = scores[r]\n",
    "                else:\n",
    "                    break\n",
    "        # 最后一个话语对\n",
    "        elif i == len(scores):\n",
    "            for l in range(i-1, -1, -1):              # 倒序遍历\n",
    "                if lflag <= scores[l]:\n",
    "                    lflag = scores[l]\n",
    "                else:\n",
    "                    break\n",
    "        # 中间话语对\n",
    "        else:\n",
    "            for r in range(i+1, len(scores)):\n",
    "                if rflag <= scores[r]:\n",
    "                    rflag = scores[r]\n",
    "                else:\n",
    "                    break\n",
    "            for l in range(i-1, -1, -1):\n",
    "                if lflag <= scores[l]:\n",
    "                    lflag = scores[l]\n",
    "                else:\n",
    "                    break\n",
    "        # 计算话语对i的深度（越大话题相关性越小）\n",
    "        depth_score = 0.5 * (lflag + rflag - 2*scores[i])\n",
    "        output_scores.append(depth_score)\n",
    "\n",
    "    return output_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aee2e6",
   "metadata": {},
   "source": [
    "加载训练好的主干模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d07ca7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForNextSentencePrediction(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyNSPHead(\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 0\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased', num_labels=2, output_attentions=False, output_hidden_states=False)\n",
    "MODEL_PATH = 'save/dailydialog/2023-05-03/weights_nspbert_dailydialog'\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.cuda(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53beaa15",
   "metadata": {},
   "source": [
    "加载测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61273ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input_docs = 'data/test_data/'\n",
    "input_files = [f for f in listdir(path_input_docs) if isfile(join(path_input_docs, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7442867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719059fb",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23753e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** The current file is : example.txt***********\n",
      "NextSentencePredictorOutput(loss=None, logits=tensor([[ 4.1146, -5.2021],\n",
      "        [ 3.1953, -4.9699],\n",
      "        [ 2.6040, -4.5784],\n",
      "        [ 5.4378, -5.4917],\n",
      "        [ 5.4696, -5.3803],\n",
      "        [ 0.3566, -0.9300],\n",
      "        [ 5.3355, -5.8745],\n",
      "        [ 3.4517, -5.0182],\n",
      "        [ 5.1997, -5.6081],\n",
      "        [ 2.1837, -3.9231],\n",
      "        [ 4.4722, -5.6384]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "tensor([[ 4.1146, -5.2021],\n",
      "        [ 3.1953, -4.9699],\n",
      "        [ 2.6040, -4.5784],\n",
      "        [ 5.4378, -5.4917],\n",
      "        [ 5.4696, -5.3803],\n",
      "        [ 0.3566, -0.9300],\n",
      "        [ 5.3355, -5.8745],\n",
      "        [ 3.4517, -5.0182],\n",
      "        [ 5.1997, -5.6081],\n",
      "        [ 2.1837, -3.9231],\n",
      "        [ 4.4722, -5.6384]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "tensor([4.1146, 3.1953, 2.6040, 5.4378, 5.4696, 0.3566, 5.3355, 3.4517, 5.1997,\n",
      "        2.1837, 4.4722], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "[0.9839306473731995, 0.9606577754020691, 0.9311175346374512, 0.9956698417663574, 0.9958047270774841, 0.5882265567779541, 0.9952054023742676, 0.9692812561988831, 0.9945119619369507, 0.8987722396850586, 0.9887064695358276]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "0\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "1\n",
      "pk:  0.4\n",
      "wd:  0.4\n",
      "mae:  2.0\n",
      "f1:  0.7777777777777777\n",
      "dp variance:  0.12100718772737692\n"
     ]
    }
   ],
   "source": [
    "c = 0                                  # 记录测试文件数\n",
    "# pick_num = 3\n",
    "score_wd = 0; score_mae = 0; score_f1 = 0; score_pk = 0\n",
    "dp_var = []\n",
    "\n",
    "# 遍历测试文件\n",
    "for file in input_files:\n",
    "\n",
    "    # 过滤文件\n",
    "    if file not in ['.DS_Store', '196']:\n",
    "    # if file not in ['.DS_Store']:\n",
    "        print('*********** The current file is : ' + file + '***********')\n",
    "        text = []                      # 记录文件句子\n",
    "        id_inputs = []                 # 记录话语对\n",
    "        depth_scores = []              # 记录话语对的深度值\n",
    "        seg_r_labels = []              # 记录真实标签\n",
    "        seg_r = []                     # 记录真实窗口\n",
    "        tmp = 0\n",
    "\n",
    "        # 遍历当前文件中的句子，记录真实标签和窗口\n",
    "        for line in open('data/test_data/'+file):\n",
    "            # 未遇到分隔符\n",
    "            if '================' not in line.strip():\n",
    "                text.append(line.strip())\n",
    "                seg_r_labels.append(0)\n",
    "                tmp += 1\n",
    "            # 遇到分隔符\n",
    "            else:\n",
    "                seg_r_labels[-1] = 1\n",
    "                seg_r.append(tmp)\n",
    "                tmp = 0\n",
    "\n",
    "        seg_r.append(tmp)\n",
    "\n",
    "        # 遍历当前文件中的句子，顺序创建话语对\n",
    "        for i in range(len(text)-1):\n",
    "            sent1 = text[i]\n",
    "            sent2 = text[i+1]\n",
    "            encoded_sent1 = tokenizer.encode(sent1, add_special_tokens=True, max_length=128, return_tensors='pt')\n",
    "            encoded_sent2 = tokenizer.encode(sent2, add_special_tokens=True, max_length=128, return_tensors='pt')\n",
    "            encoded_pair = encoded_sent1[0].tolist() + encoded_sent2[0].tolist()[1:]\n",
    "            id_inputs.append(torch.Tensor(encoded_pair))\n",
    "\n",
    "        # 固定话语对表征的维度\n",
    "        MAX_LEN = 256\n",
    "        id_inputs = pad_sequences(id_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "        # 构建各话语对的mask矩阵\n",
    "        attention_masks = []\n",
    "        for sent in id_inputs:\n",
    "            att_mask = [int(token_id > 0) for token_id in sent]\n",
    "            attention_masks.append(att_mask)\n",
    "\n",
    "        # 获得测试输入\n",
    "        test_inputs = torch.tensor(id_inputs).to(device)\n",
    "        test_masks = torch.tensor(attention_masks).to(device)\n",
    "\n",
    "        # 话语对前向传播\n",
    "        scores = model(test_inputs, attention_mask=test_masks)\n",
    "        print(scores)\n",
    "        print('%'*100)\n",
    "        print(scores[0])\n",
    "        print('%'*100)\n",
    "        print(scores[0][:, 0])\n",
    "        print('%'*100)\n",
    "        # 获取：[CLS]输出 ——> 预测为类别0的概率 ——> 激活\n",
    "        scores = torch.sigmoid(scores[0][:, 0]).detach().cpu().numpy().tolist()\n",
    "        print(scores)\n",
    "        print('%'*100)\n",
    "\n",
    "        # 计算话语对的深度\n",
    "        depth_scores = depth_score_cal(scores)\n",
    "        # print(depth_scores)\n",
    "\n",
    "        # boundary_indice = np.argsort(np.array(depth_scores))[-pick_num:]\n",
    "\n",
    "        # 计算分割阈值\n",
    "        threshold = sum(depth_scores)/(len(depth_scores))-0.1*statistics.stdev(depth_scores)\n",
    "        dp_var.append(statistics.stdev(depth_scores))\n",
    "        \n",
    "        # 记录分割点\n",
    "        boundary_indice = []\n",
    "        # 记录预测标签\n",
    "        seg_p_labels = [0]*(len(depth_scores)+1)\n",
    "\n",
    "        # 遍历话语对的深度值\n",
    "        for i in range(len(depth_scores)):\n",
    "            # 若深度值大于阈值，确定为分割点\n",
    "            if depth_scores[i] > threshold:\n",
    "                boundary_indice.append(i)\n",
    "        # 遍历分割点，修改预测标签\n",
    "        for i in boundary_indice:\n",
    "            seg_p_labels[i] = 1\n",
    "\n",
    "        # 记录预测窗口\n",
    "        tmp = 0; seg_p = []\n",
    "        # 遍历预测标签，添加预测窗口\n",
    "        for fake in seg_p_labels:\n",
    "            if fake == 1:\n",
    "                tmp += 1\n",
    "                seg_p.append(tmp)\n",
    "                tmp = 0\n",
    "            else:\n",
    "                tmp += 1\n",
    "        seg_p.append(tmp)\n",
    "\n",
    "        # print(depth_scores)\n",
    "        # print(threshold)\n",
    "        # print(seg_p)\n",
    "        # print(seg_r)\n",
    "\n",
    "        score_pk += pk(seg_p, seg_r)\n",
    "        score_wd += wd(seg_p, seg_r)\n",
    "        score_mae += sum(list(map(abs, np.array(seg_r_labels)-np.array(seg_p_labels))))\n",
    "        score_f1 += f1_score(seg_r_labels, seg_p_labels, labels=[0, 1], average='macro')\n",
    "        print(c)\n",
    "        print(seg_r_labels)\n",
    "        print(seg_p_labels)\n",
    "        c += 1\n",
    "\n",
    "print(c)\n",
    "print('pk: ', score_pk/c)\n",
    "print('wd: ', score_wd/c)\n",
    "print('mae: ', score_mae/c)\n",
    "print('f1: ', score_f1/c)\n",
    "print('dp variance: ', sum(dp_var)/c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd29665c",
   "metadata": {},
   "source": [
    "中间变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65336352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, I need to check my VA claim.',\n",
       " 'Of course. You can check the status of a VA claim or appeal online.',\n",
       " 'Great. Can I use this tool?',\n",
       " 'Do you have any free accounts?',\n",
       " 'Yes, I have a Premiun My Health Vet account.',\n",
       " 'Cool. That is enough to be able to use this tool.',\n",
       " \"I don't see a document I sent to VA as evidence. Can you help me?\",\n",
       " 'Yes. Of course.',\n",
       " 'Can I upload documents online to support your initial claim?',\n",
       " 'Yes. You can.',\n",
       " 'Can I use this tool to check the status of a claim or appeal for VA health cares?',\n",
       " 'Yes. Of course.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c60b19d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 101, 7592, 1010, ...,    0,    0,    0],\n",
       "        [ 101, 1997, 2607, ...,    0,    0,    0],\n",
       "        [ 101, 2307, 1012, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2064, 1045, ...,    0,    0,    0],\n",
       "        [ 101, 2748, 1012, ...,    0,    0,    0],\n",
       "        [ 101, 2064, 1045, ...,    0,    0,    0]]),\n",
       " 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_inputs, len(id_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b94ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  7592,  1010,  1045,  2342,  2000,  4638,  2026, 12436,\n",
       "        4366,  1012,   102,  1997,  2607,  1012,  2017,  2064,  4638,\n",
       "        1996,  3570,  1997,  1037, 12436,  4366,  2030,  5574,  3784,\n",
       "        1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f5f4666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1997,  2607,  1012,  2017,  2064,  4638,  1996,  3570,\n",
       "        1997,  1037, 12436,  4366,  2030,  5574,  3784,  1012,   102,\n",
       "        2307,  1012,  2064,  1045,  2224,  2023,  6994,  1029,   102,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c12de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 2, 2, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f274f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 4, 1, 2, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094145e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch113]",
   "language": "python",
   "name": "conda-env-pytorch113-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
